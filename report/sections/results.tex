%!TEX root = ../paper.tex
\documentclass[../paper.tex]{subfiles}

\begin{document}

\section{Results and Analysis}
As the initially planned experiment proved to be beyond the constraints of this research, the results, analysis, and discussion will only cover Case 1.

Initially the original experiment and the cases were conducted under the assumption that parameter estimation in the underlying models was low variance.
Accordingly, replication of runs served to help reduce the noise in the measurement with experiments run with between 5 and 50 replications.
While there was little information about what to expect from the Invert method, the common understanding of OLS model behavior leads that the estimates of a coefficient for the main effects using the Drop method should show no bias.

The bias for a run with 5 replications can be seen in Figure \ref{fig:drop_invert_bias_demo_low_rep}. In this particular run of the experiment the bias for $\hat{\beta}_{X_{1}}$ using the Invert and Drop methods appears to be 8\% and -8\% respectively.

\begin{figure}[H]

\centering
\includegraphics[width=.8\textwidth]{drop_invert_bias_demo_low_rep}
%\caption{Example of a parametric plot ($\sin (x), \cos(x), x$)}
\caption{$\hat{\beta}_{1}$ bias  for Invert and Drop methods -- 5 Replications}
\label{fig:drop_invert_bias_demo_low_rep}
\end{figure}


The observation of non-zero bias in the Drop method triggered a full review of the experimental implementation.
The data generation and fit code was replicated in R. The bias behavior was observed in the second implementation as well.

%In the presence of moderate noise in the underlying datasets, the models

The simplest solution to this problem was a significant increase in the number of replications for each run.
Through trial and error it was determined that between 400 and 1000 replications was required to ensure that Drop method beta estimates converged correctly. Figures \ref{fig:drop_invert_bias_demo2} and \ref{fig:drop_invert_bias_demo3} show improved convergence with increased replications. In these plots as with the plot above, the measure of bias is the percentage difference between the coefficient estimate and the true value.

\begin{figure}[H]

\centering
\includegraphics[width=.8\textwidth]{drop_invert_bias_demo2}
%\caption{Example of a parametric plot ($\sin (x), \cos(x), x$)}
\caption{$\hat{\beta}_{1}$ bias for Invert and Drop methods -- 101 Replications}
\label{fig:drop_invert_bias_demo2}
\end{figure}

\begin{figure}[H]

\centering
\includegraphics[width=.8\textwidth]{drop_invert_bias_demo3}
%\caption{Example of a parametric plot ($\sin (x), \cos(x), x$)}
\caption{$\hat{\beta}_{1}$  bias for Invert and Drop methods -- 501 Replications}
\label{fig:drop_invert_bias_demo3}
\end{figure}

Given the concern, a one sample two tail T Test was performed to check the mean of the bias against a null hypothesis that the mean is zero.
This test was performed on two otherwise identical sets of 101 replications and one otherwise identical set with 501 replications.
In the 101 set, the p-value was 0.470 and as such there is not enough evidence to conclude the mean is not zero as a confidence level of 0.05.
In another 101 set, the p-value was 4.064e-06 and as such there is enough evidence to reject the null at a confidence level of 0.05 and conclude that the mean was not zero.
In the 501 set, the p-value was 0.000339 also providing enough evidence at a confidence level of 0.05 to conclude that the mean was not zero.

These tests leave any conclusions about non-zero bias in doubt as it is difficult to conclude zero bias even with extensive replication. A comparison between the Drop, Invert, and Mean methods can be conducted mathematically and inspected visually. The reader must refrain from acting on any of the observations discussed in this paper until the Drop method bias issue is resolved or adequately explained.

\subsection{Model Degradation Behavior}
We will continue the discussion on model degradation using the 501 replication set.
A natural place to begin a conversation on model performance is $R^2$.
The plot of $R^2$ as a function of the percentage of missing data for the Invert, Mean, and Drop methods can be seeing in Figure \ref{fig:drop_invert_mean_r2_1}.

\begin{figure}[H]
\centering
\includegraphics[width=.8\textwidth]{drop_invert_mean_r2_1}
%\caption{Example of a parametric plot ($\sin (x), \cos(x), x$)}
\caption{$R^2$ for Invert, Drop, and Mean methods}
\label{fig:drop_invert_mean_r2_1}
\end{figure}

Next, let's examine the confidence interval range for $\hat{\beta}_{1}$. The Invert method is forcing the missing data on to model describing the complete observations. The imputed data will artificially reduce the noise producing tighter coefficient estimates around the estimates derived from the complete observations. The estimate around which the interval converges may be subject to increased bias however the issues previously discussed prevent any definitive conclusions here.
The CI width when dropping samples appears to increase with the square of the percentage of missing data. This too follows as, with fewer samples, there is less certainty in the $\beta$ estimation. The plot of the confidence interval range for all three imputation methods can be seen in Figure \ref{fig:drop_invert_mean_c1_rng_1}.

\begin{figure}[H]
\centering
\includegraphics[width=.8\textwidth]{drop_invert_mean_ci_rng_1}
%\caption{Example of a parametric plot ($\sin (x), \cos(x), x$)}
\caption{Confidence Interval range for $\hat{\beta}_{1}$ for Invert, Drop, and Mean methods}
\label{fig:drop_invert_mean_c1_rng_1}
\end{figure}

Unlike the two previous model metrics, relative MSE of prediction measures prediction of unseen data rather than model fit. Mean imputation severely degrades predictive power such that visualizing all three methods together is of little value. The three methods together can be see in Figure \ref{fig:drop_invert_mean_rel_mse_1}.

\begin{figure}[H]
\centering
\includegraphics[width=.8\textwidth]{drop_invert_mean_reL_mse_1}
%\caption{Example of a parametric plot ($\sin (x), \cos(x), x$)}
\caption{Relative MSE for Invert, Drop, and Mean methods}
\label{fig:drop_invert_mean_rel_mse_1}
\end{figure}

When the Mean method is removed we get a clearer view of of the remaining two methods.
It appears that both methods retain the predictive power of the model as the percentage of missing data decreases.
If the Invert method introduces bias then the predictive power should eventually deviate from the Drop method.
This hypothesis remains unconfirmed in this work.

The findings on the effect mean imputation on all three model performance measurements concurs with the findings of Hasan, Haliza, et al discussed in the introduction.

\begin{figure}[H]
\centering
%\includesvg{drop_invert_mean_reL_mse_2}
\includegraphics[width=.8\textwidth]{drop_invert_mean_reL_mse_2}
%\caption{Example of a parametric plot ($\sin (x), \cos(x), x$)}
\caption{Relative MSE for Invert and Drop methods}
\label{fig:drop_invert_mean_rel_mse_2}
\end{figure}


Once again, the reader must refrain from acting on any of the assertions in this section as the bias and underlying model issues remain.

\end{document}
